{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "n this notebook, we will train a [Causal Language Model](https://huggingface.co/docs/transformers/v4.41.2/en/tasks/language_modeling#causal-language-modeling) using the Hugging Face [Transformers library](https://huggingface.co/docs/transformers/en/index). We wll use the run_clm.py script to fine-tune the [GPT-2](https://huggingface.co/openai-community/gpt2) model on a custom dataset which is already available on [Hugging Face Hub](https://huggingface.co/datasets)"
      ],
      "metadata": {
        "id": "rLAVXGt-kh5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLbtNsAOkeAF",
        "outputId": "f47ff912-0365-44ba-b259-136bc7b2b242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet -U huggingface_hub transformers accelerate evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTeOV9Z7nPTU",
        "outputId": "ff99df58-e1c5-4a3d-a22e-436bf5755f8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJvEe8Egnm-E",
        "outputId": "d2e53b74-3da2-4a64-e94a-dc8c4d4db3fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.41.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "yoObPT8Tp_Q7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "cLAizE-2qDYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HT5mlnhLn30F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test runs"
      ],
      "metadata": {
        "id": "DG5pgNM6nnft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "set_seed(47)"
      ],
      "metadata": {
        "id": "CULvi-g3obyp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "nF-0DbYJnq3Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIuCiZjtpZzA",
        "outputId": "945a2625-c9e3-43ac-9ced-36044d10ba77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, one of the most important languages, I use the GNU C# Language API on my workstation, and it\"},\n",
              " {'generated_text': \"Hello, I'm a language model, but it's not like every single part of my project is actually a language model—it seems much a mix\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and you must be a language model too.\\n\\nMy goal is to create a framework that allows many different\"},\n",
              " {'generated_text': \"Hello, I'm a language model, my program is a syntax model.\\n\\nWhat is it that makes it so that I can understand more complex\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a programmer. I'm teaching all types in one language: PHP. No, I'm not just putting\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fR7EwfdZoGFg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "NK9K0H1XoHjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data"
      ],
      "metadata": {
        "id": "6ydjWA1qpsZa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path openai-community/gpt2 \\\n",
        "    --dataset_name ndamulelonemakh/zabantu-data \\\n",
        "    --dataset_config_name eng \\\n",
        "    --validation_split_percentage 10 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir data/zaf-gpt2-v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTTEYQdpoHQZ",
        "outputId": "88192419-ac75-4246-b612-5836b90eb66c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-28 10:11:18.200346: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-28 10:11:18.200403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-28 10:11:18.201731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-28 10:11:19.284412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/28/2024 10:11:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/28/2024 10:11:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=data/zaf-gpt2-v1/runs/May28_10-11-22_4b5427c46435,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=data/zaf-gpt2-v1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=data/zaf-gpt2-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "05/28/2024 10:11:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "05/28/2024 10:11:24 - INFO - datasets.builder - Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "05/28/2024 10:11:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "05/28/2024 10:11:25 - INFO - datasets.builder - Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "05/28/2024 10:11:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "05/28/2024 10:11:26 - INFO - datasets.builder - Found cached dataset zabantu-data (/root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "05/28/2024 10:11:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402\n",
            "[INFO|configuration_utils.py:733] 2024-05-28 10:11:26,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-05-28 10:11:26,835 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"openai-community/gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.42.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-05-28 10:11:26,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-05-28 10:11:26,884 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"openai-community/gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.42.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,894 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,894 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,894 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,894 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,895 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2109] 2024-05-28 10:11:26,895 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-05-28 10:11:26,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-05-28 10:11:26,896 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"openai-community/gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.42.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3477] 2024-05-28 10:11:27,003 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "[INFO|configuration_utils.py:993] 2024-05-28 10:11:27,011 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4283] 2024-05-28 10:11:27,399 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4291] 2024-05-28 10:11:27,399 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at openai-community/gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:948] 2024-05-28 10:11:27,448 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
            "[INFO|configuration_utils.py:993] 2024-05-28 10:11:27,449 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-eb416162a8c95568.arrow\n",
            "05/28/2024 10:11:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-eb416162a8c95568.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-8f7b0e02ee33495e.arrow\n",
            "05/28/2024 10:11:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-8f7b0e02ee33495e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-082d840cf873adf1.arrow\n",
            "05/28/2024 10:11:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-082d840cf873adf1.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-db8061dc528062fa.arrow\n",
            "05/28/2024 10:11:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/ndamulelonemakh___zabantu-data/eng/0.0.0/e3dc41ed6032936df16618d905c08748c0298402/cache-db8061dc528062fa.arrow\n",
            "[INFO|trainer.py:2108] 2024-05-28 10:11:28,525 >> ***** Running training *****\n",
            "[INFO|trainer.py:2109] 2024-05-28 10:11:28,525 >>   Num examples = 1,521\n",
            "[INFO|trainer.py:2110] 2024-05-28 10:11:28,525 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2111] 2024-05-28 10:11:28,525 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2114] 2024-05-28 10:11:28,525 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2115] 2024-05-28 10:11:28,525 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2116] 2024-05-28 10:11:28,525 >>   Total optimization steps = 1,143\n",
            "[INFO|trainer.py:2117] 2024-05-28 10:11:28,526 >>   Number of trainable parameters = 124,439,808\n",
            "{'loss': 3.3167, 'grad_norm': 3.6144022941589355, 'learning_rate': 2.8127734033245845e-05, 'epoch': 1.31}\n",
            " 44% 500/1143 [12:21<16:25,  1.53s/it][INFO|trainer.py:3444] 2024-05-28 10:23:50,264 >> Saving model checkpoint to data/zaf-gpt2-v1/checkpoint-500\n",
            "[INFO|configuration_utils.py:472] 2024-05-28 10:23:50,265 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:762] 2024-05-28 10:23:50,266 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:2621] 2024-05-28 10:23:52,120 >> Model weights saved in data/zaf-gpt2-v1/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2519] 2024-05-28 10:23:52,121 >> tokenizer config file saved in data/zaf-gpt2-v1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2528] 2024-05-28 10:23:52,121 >> Special tokens file saved in data/zaf-gpt2-v1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.948, 'grad_norm': 3.835808515548706, 'learning_rate': 6.255468066491689e-06, 'epoch': 2.62}\n",
            " 87% 1000/1143 [25:24<03:38,  1.53s/it][INFO|trainer.py:3444] 2024-05-28 10:36:52,680 >> Saving model checkpoint to data/zaf-gpt2-v1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:472] 2024-05-28 10:36:52,681 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:762] 2024-05-28 10:36:52,682 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2621] 2024-05-28 10:37:11,268 >> Model weights saved in data/zaf-gpt2-v1/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2519] 2024-05-28 10:37:11,269 >> tokenizer config file saved in data/zaf-gpt2-v1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2528] 2024-05-28 10:37:11,269 >> Special tokens file saved in data/zaf-gpt2-v1/checkpoint-1000/special_tokens_map.json\n",
            "100% 1143/1143 [29:23<00:00,  1.20s/it][INFO|trainer.py:3444] 2024-05-28 10:40:51,767 >> Saving model checkpoint to data/zaf-gpt2-v1/checkpoint-1143\n",
            "[INFO|configuration_utils.py:472] 2024-05-28 10:40:51,768 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-1143/config.json\n",
            "[INFO|configuration_utils.py:762] 2024-05-28 10:40:51,769 >> Configuration saved in data/zaf-gpt2-v1/checkpoint-1143/generation_config.json\n",
            "[INFO|modeling_utils.py:2621] 2024-05-28 10:41:42,019 >> Model weights saved in data/zaf-gpt2-v1/checkpoint-1143/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2519] 2024-05-28 10:41:42,020 >> tokenizer config file saved in data/zaf-gpt2-v1/checkpoint-1143/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2528] 2024-05-28 10:41:42,020 >> Special tokens file saved in data/zaf-gpt2-v1/checkpoint-1143/special_tokens_map.json\n",
            "[INFO|trainer.py:2358] 2024-05-28 10:42:19,398 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1850.8724, 'train_samples_per_second': 2.465, 'train_steps_per_second': 0.618, 'train_loss': 3.100747042544975, 'epoch': 3.0}\n",
            "100% 1143/1143 [30:50<00:00,  1.62s/it]\n",
            "[INFO|trainer.py:3444] 2024-05-28 10:42:19,433 >> Saving model checkpoint to data/zaf-gpt2-v1\n",
            "[INFO|configuration_utils.py:472] 2024-05-28 10:42:19,436 >> Configuration saved in data/zaf-gpt2-v1/config.json\n",
            "[INFO|configuration_utils.py:762] 2024-05-28 10:42:19,436 >> Configuration saved in data/zaf-gpt2-v1/generation_config.json\n",
            "[INFO|modeling_utils.py:2621] 2024-05-28 10:42:27,586 >> Model weights saved in data/zaf-gpt2-v1/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2519] 2024-05-28 10:42:27,587 >> tokenizer config file saved in data/zaf-gpt2-v1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2528] 2024-05-28 10:42:27,587 >> Special tokens file saved in data/zaf-gpt2-v1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2220786GF\n",
            "  train_loss               =     3.1007\n",
            "  train_runtime            = 0:30:50.87\n",
            "  train_samples            =       1521\n",
            "  train_samples_per_second =      2.465\n",
            "  train_steps_per_second   =      0.618\n",
            "05/28/2024 10:42:27 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3753] 2024-05-28 10:42:27,638 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3755] 2024-05-28 10:42:27,638 >>   Num examples = 185\n",
            "[INFO|trainer.py:3758] 2024-05-28 10:42:27,638 >>   Batch size = 4\n",
            "100% 47/47 [00:21<00:00,  2.17it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.4422\n",
            "  eval_loss               =     2.8837\n",
            "  eval_runtime            = 0:00:22.16\n",
            "  eval_samples            =        185\n",
            "  eval_samples_per_second =      8.348\n",
            "  eval_steps_per_second   =      2.121\n",
            "  perplexity              =    17.8811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Troubleshoot transformers version\n",
        "# !pip uninstall transformers -y\n",
        "# !git clone https://github.com/huggingface/transformers --depth 1\n",
        "# !cd transformers && pip install ."
      ],
      "metadata": {
        "id": "lSmAgtAuoJpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvehNalYoJjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nB9bYK0qoJfx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test New Model"
      ],
      "metadata": {
        "id": "1Y1RWe1W1M1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "7wvoMGkq10t7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = GPT2LMHeadModel.from_pretrained('./data/zaf-gpt2-v1')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "cT0f0fFP14md"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_new = pipeline('text-generation', model=new_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "y7kQLjRK1ORy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_new(\"The president of South Africa,\",\n",
        "          max_length=30,\n",
        "          num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz051_8p2uXj",
        "outputId": "728ae544-e4f4-4742-af72-c60cfe7bdf94"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The president of South Africa, Dr Bimbo Mpumalanga, congratulated his supporters on the results and thanked them for their hard work in'},\n",
              " {'generated_text': 'The president of South Africa, President Jacob Zuma will lead delegation of Ministers to South Africa from 3 to 4 August 2007 to attend an African Union ('},\n",
              " {'generated_text': 'The president of South Africa, Dr Thabo Mbeki, will deliver the South African National Debate Programme at the SANDF Summit in Durban on'},\n",
              " {'generated_text': 'The president of South Africa, Mr Nair, will address the conference on the 25th anniversary of the arrest and conviction of apartheid leader Nelson Mandela on'},\n",
              " {'generated_text': 'The president of South Africa, Dr Mark Lekgotla, took this moment to congratulate South Africa on the contribution it made to Africa since democracy.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator_new(\"The springboks coach,\",\n",
        "          max_length=300,\n",
        "          num_return_sequences=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WxFuLoy2NX3",
        "outputId": "b46f26ef-7237-4971-cad4-682d863c9d07"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"The springboks coach, Tshwane Pathanathwa, who won the men's 100m freestyle relay in the 100m final of the men's women's 100m butterfly final in the 100m freestyle final.Mr Molotswane Kwa-Pini as the Chief Financial Officer of the Public Provinces Water Board.Mr Mabuzini Htatshu as the Deputy Chairperson of the Department of Water Affairs.South African Airways, which will receive an additional R300 million for the fourth quarter of 2016/17, will become the fourth carrier to roll out hybrid passenger services while the country remains committed to supporting the country economy through low cost of fuel and sustainable use of natural resources.Issues In the Environment.It will be able to meet the high demand scenario and is expected to reach capacity before the end of.The NOPMA is an annual report made annually to the South African National Police Advisory Council and was commissioned to be submitted to the President in May.Dr Gail MzwuluMandela Zondo as the Chief Financial Officer of the National Water Board.Mamadou Diallo as the Deputy Director-General (DG): Trade.Nyuba City Tourism BoardMr Tshwane Nkulaka as the Deputy Director-General (DDG): Land.Ms Anele Nyama (Chairperson)Cabinet Spokesperson Mobile: 083 501 0139Cabinet appeals to all\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}