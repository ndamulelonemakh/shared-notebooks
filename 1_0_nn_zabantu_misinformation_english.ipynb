{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndamulelonemakh/shared-notebooks/blob/main/1_0_nn_zabantu_misinformation_english.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "* This notebook demonstrates how to fine-tune any [XLM-RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta#xlm-roberta) model for text classification.\n",
        "* We will [ZaBantu](https://huggingface.co/dsfsi/zabantu-xlm-roberta) pre-trained model by default, but you can change this to any model trained on the [XLM-R architecture](https://arxiv.org/abs/1911.02116)\n",
        "\n",
        "\n",
        "---\n",
        "* Environment Setup\n",
        "* Global Parameters\n",
        "* Data Preparation\n",
        "* Preprocessing the data\n",
        "* Fine-tuning the Pre-Trained Model\n",
        "* Evaluate the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "Xb3zcfIGjsXA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQj_vYk7lSh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "oof2GXrQlUot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet transformers  datasets sentencepiece evaluate huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlsfvAMlXGu",
        "outputId": "241b3ea9-2445-47e1-923b-e9d23b487948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install accelerate -U --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L4UZ2sFlXDt",
        "outputId": "7ec0e569-6bf1-4fac-d3e4-38fc1ea04228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/297.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You may need to restart the kernel before  proceeding to the next steps"
      ],
      "metadata": {
        "id": "tKggP_PdlgOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "PWAYWD99lXAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If your pre-trained **model is private**, uncomment the code below:"
      ],
      "metadata": {
        "id": "a-6f2JjGlu5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login, whoami\n",
        "\n",
        "# try:\n",
        "#   whoami()\n",
        "# except:\n",
        "#   print(\"User token not found, calling notebook_login()...\")\n",
        "#   notebook_login()"
      ],
      "metadata": {
        "id": "ddEBWRMolqpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "CaQfL7m3l7dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "20TazYexl7Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Settings"
      ],
      "metadata": {
        "id": "-bOpOD--l79A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"misinformation\" # Can be any single label text classification task\n",
        "\n",
        "# model options:\n",
        "# - dsfsi/zabantu-xlm-roberta\n",
        "# - dsfsi/zabantu-sot-ven-170m\n",
        "# - dsfsi/zabantu-nso-ven-170m\n",
        "# - dsfsi/zabantu-nso-120m\n",
        "# - FacebookAI/xlm-roberta-base\n",
        "model_checkpoint = \"dsfsi/zabantu-sot-ven-170m\"\n",
        "dataset_checkpoint = \"dfsi/misinformation-english\"\n",
        "local_data_path = \"local_data\"\n",
        "\n",
        "push_to_hub_enabled = False\n",
        "trained_model_checkpoint = f\"{model_checkpoint}-finetuned-{task}\"\n",
        "trained_model_checkpoint_hub = f\"ndamulelonemakh/{trained_model_checkpoint}\"\n",
        "batch_size = 16  # adjust depending on GPU size\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "N7nq_nbDl7XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73n_elJKl7UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9aCMiiXmZBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preperation\n",
        "\n",
        "Once processed, your dataset must look somthing like this:"
      ],
      "metadata": {
        "id": "bc-19AiHmZTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load raw data"
      ],
      "metadata": {
        "id": "pFBbdNftqnwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n \"true( English data).zip\" -d local_data\n",
        "!unzip -n \"fake (English data).zip\" -d local_data"
      ],
      "metadata": {
        "id": "zr8vOOSymY_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect data\n",
        "!head -n 3 local_data/True.csv"
      ],
      "metadata": {
        "id": "WueA3ZDJo9Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 local_data/Fake.csv"
      ],
      "metadata": {
        "id": "tWzEomgdpDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tranform to DataFrame"
      ],
      "metadata": {
        "id": "b7aU2R-fqq9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_merge(fake_data_file: str, true_data_file: str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Loads fake and true data from their respective files and merges them into a single DataFrame.\n",
        "\n",
        "  Args:\n",
        "    fake_data_file: Path to the fake data file.\n",
        "    true_data_file: Path to the true data file.\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame containing the merged data.\n",
        "  \"\"\"\n",
        "\n",
        "  fake_data = pd.read_csv(os.path.join(local_data_path, fake_data_file))\n",
        "  fake_data[\"label\"] = \"FAKE\"\n",
        "  true_data = pd.read_csv(os.path.join(local_data_path, true_data_file))\n",
        "  true_data[\"label\"] = \"TRUE\"\n",
        "\n",
        "  merged_data = pd.concat([fake_data, true_data], ignore_index=True)\n",
        "\n",
        "  return merged_data"
      ],
      "metadata": {
        "id": "bmEMpzFUmhL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = load_and_merge(\"Fake.csv\", \"True.csv\")\n",
        "print(raw_df.info())\n",
        "raw_df.sample(3)"
      ],
      "metadata": {
        "id": "dZseVc_wmhIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.subject.value_counts().plot(kind=\"barh\")"
      ],
      "metadata": {
        "id": "telpT0CRmhEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.label.value_counts(True)"
      ],
      "metadata": {
        "id": "m3hyipCimhBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCx-uc3urOTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufEglKWRqxAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convert to Transformers [Datasets](https://huggingface.co/docs/datasets/en/index)"
      ],
      "metadata": {
        "id": "tUuu8vgQqxSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, ClassLabel, Sequence, DatasetDict, Value,Features"
      ],
      "metadata": {
        "id": "WCngZyt1r9-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: complete this python function implementation: from datasets import Dataset, ClassLabel, Sequence, DatasetDict\n",
        "# def pandas_to_huggingface_dataset(df: pd.DataFrame, split=True) -> DatasetDict:\n",
        "#   pass\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, ClassLabel, Sequence, DatasetDict\n",
        "\n",
        "def pandas_to_huggingface_dataset(df: pd.DataFrame,\n",
        "                                  split=True) -> DatasetDict:\n",
        "  \"\"\"\n",
        "  Converts a Pandas DataFrame to a Hugging Face Dataset.\n",
        "\n",
        "  Args:\n",
        "    df: The Pandas DataFrame to convert.\n",
        "    split: Whether to split the data into train and test sets.\n",
        "\n",
        "  Returns:\n",
        "    A Hugging Face DatasetDict containing the converted data.\n",
        "  \"\"\"\n",
        "\n",
        "  features = Features({\n",
        "      \"text\": Value(dtype=\"string\"),\n",
        "      \"label\": ClassLabel(names=[\"FAKE\", \"TRUE\"]),\n",
        "  })\n",
        "  df = df[[\"text\", \"label\"]]  # we do not care about the rest for now\n",
        "  dataset = Dataset.from_pandas(df, features=features)\n",
        "\n",
        "  if split:\n",
        "    return dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "  else:\n",
        "    return dataset\n",
        "\n",
        "dataset = pandas_to_huggingface_dataset(raw_df)\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "szdXyigwrdSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "Sryn-wQBs0gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].features"
      ],
      "metadata": {
        "id": "NwHursXNrMYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISJKHCV6qiKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1B8eKVRrUh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "2FQzNhWWtIVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "VT1PDTEPtREy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
      ],
      "metadata": {
        "id": "_61dzo1ftN09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check which type of models have a fast tokenizer available and which don't on the big table of models.\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "metadata": {
        "id": "Ew1x6AvUtNr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Muthu munwe na munwe una mukovhe ware!\")"
      ],
      "metadata": {
        "id": "IO9ZFhPKtief"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('===Subword Tokenisation Illustration===')\n",
        "example = dataset[\"train\"][4]\n",
        "print('=' * 50 + '\\n')\n",
        "print(\"ORIGINAL TEXT\")\n",
        "print(example[\"text\"])\n",
        "print('-' * 20   + '\\n')\n",
        "print('TOKENS:')\n",
        "tokenized_input = tokenizer(example[\"text\"], is_split_into_words=False)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Q5e96pGTtr9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "nq2c-8UwudGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "3X9DBUKMuHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets['test'][0])"
      ],
      "metadata": {
        "id": "9sdai1lXwNeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DI7Z1R_4uHEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "9ZYymU5ruHyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "EB9JfPJxuHBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label-to-id mapping\n",
        "label_names =  dataset['train'].features['label'].names\n",
        "\n",
        "label2id = {label_name:i for i, label_name in enumerate(label_names)}\n",
        "id2label = {id: label for label, id in label2id.items()}\n",
        "label2id"
      ],
      "metadata": {
        "id": "g23XUG2-uqn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
        "                                                        num_labels=len(label_names),\n",
        "                                                        id2label=id2label,\n",
        "                                                        label2id=label2id)"
      ],
      "metadata": {
        "id": "CRiGHunouqkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    trained_model_checkpoint,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=push_to_hub_enabled,\n",
        ")"
      ],
      "metadata": {
        "id": "SXlMXsPbr3Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define collator to enable dynamic text padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "6nUrhKV6vEO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "Qm0QoMxSvEMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "X8KfBUhwvEJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "86JorTUUvYQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lx0NB4bvEGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Fine-tuned Model"
      ],
      "metadata": {
        "id": "0COUxGfFvir6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "57fE3VCUvj-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if push_to_hub_enabled:\n",
        "  trainer.push_to_hub()\n",
        "else:\n",
        "  trainer.save_model(trained_model_checkpoint)"
      ],
      "metadata": {
        "id": "nQyI_WXCvj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4mC1dMXwvwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage of your trained model using Huggingface pipelines"
      ],
      "metadata": {
        "id": "ChLf2OBVvxxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "PFoTn4hPvwat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline('token-classification', model=trained_model_checkpoint_hub if push_to_hub_enabled else trained_model_checkpoint)"
      ],
      "metadata": {
        "id": "KplEHyEevweN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_example = \"Barack Hussein Obama was born in soweto. He is the only sibling to Donald Trump. He became the first black president in South Africa\"\n",
        "pipe(test_example)"
      ],
      "metadata": {
        "id": "W0zpUX6vv8Od"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}